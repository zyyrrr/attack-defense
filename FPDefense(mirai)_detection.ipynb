{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vector_len = 968 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: -0.089181\n",
      "dg_loss: -0.116909\n",
      "threshold: 0.027728\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9055\n",
      "F-score: 0.9504\n",
      "10 [D real_err: -0.088375, fake_err: -0.115857] [G loss: -0.116930]\n",
      "d_loss: -0.123233\n",
      "dg_loss: -0.161909\n",
      "threshold: 0.038676\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9125\n",
      "F-score: 0.9543\n",
      "20 [D real_err: -0.122109, fake_err: -0.161197] [G loss: -0.161906]\n",
      "d_loss: -0.152877\n",
      "dg_loss: -0.197812\n",
      "threshold: 0.044935\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9192\n",
      "F-score: 0.9579\n",
      "30 [D real_err: -0.152122, fake_err: -0.197132] [G loss: -0.197815]\n",
      "d_loss: -0.187412\n",
      "dg_loss: -0.245279\n",
      "threshold: 0.057867\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9252\n",
      "F-score: 0.9611\n",
      "40 [D real_err: -0.186451, fake_err: -0.244080] [G loss: -0.245328]\n",
      "d_loss: -0.231224\n",
      "dg_loss: -0.317487\n",
      "threshold: 0.086263\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9257\n",
      "F-score: 0.9614\n",
      "50 [D real_err: -0.231154, fake_err: -0.315947] [G loss: -0.317548]\n",
      "d_loss: -0.256427\n",
      "dg_loss: -0.366277\n",
      "threshold: 0.109850\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9255\n",
      "F-score: 0.9613\n",
      "60 [D real_err: -0.263264, fake_err: -0.365865] [G loss: -0.366341]\n",
      "d_loss: -0.263874\n",
      "dg_loss: -0.382368\n",
      "threshold: 0.118495\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9240\n",
      "F-score: 0.9605\n",
      "70 [D real_err: -0.269558, fake_err: -0.382124] [G loss: -0.382434]\n",
      "d_loss: -0.270121\n",
      "dg_loss: -0.392462\n",
      "threshold: 0.122341\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9230\n",
      "F-score: 0.9600\n",
      "80 [D real_err: -0.269163, fake_err: -0.392347] [G loss: -0.392478]\n",
      "d_loss: -0.275610\n",
      "dg_loss: -0.399273\n",
      "threshold: 0.123663\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9223\n",
      "F-score: 0.9596\n",
      "90 [D real_err: -0.277363, fake_err: -0.399200] [G loss: -0.399311]\n",
      "d_loss: -0.280572\n",
      "dg_loss: -0.404094\n",
      "threshold: 0.123522\n",
      "len of dg_loss: 6001.000000\n",
      "Precision: 1.0000\n",
      "Recall: 0.9212\n",
      "F-score: 0.9590\n",
      "100 [D real_err: -0.287068, fake_err: -0.404035] [G loss: -0.404127]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(125)\n",
    "\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=1)\n",
    "        \n",
    "class Generator(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map4 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        return self.f(self.map4(x))\n",
    "\n",
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        return self.map3(x)\n",
    "\n",
    "def load_data(filename):\n",
    "    x = []\n",
    "    with open(filename,'r') as data:\n",
    "        lines = data.readlines()\n",
    "        for line in lines:\n",
    "            line=line.strip('\\n')\n",
    "            x.append(list(line))\n",
    "    x = np.array(x,dtype=np.float64)\n",
    "    return x\n",
    "\n",
    "# Model parameters\n",
    "feature_dims = max_vector_len\n",
    "z_dims = 20\n",
    "g_input_size = feature_dims + z_dims\n",
    "g_output_size = feature_dims\n",
    "g_hidden_size = int(math.sqrt(g_input_size+g_output_size)+10)\n",
    "d_input_size = g_output_size\n",
    "d_output_size = 1\n",
    "d_hidden_size = int(math.sqrt(d_input_size+d_output_size)+10)\n",
    "\n",
    "# 100*32*5=16000<30000*0.8=24000\n",
    "epochs = 100 \n",
    "batch_size = 32\n",
    "print_interval = 10\n",
    "\n",
    "d_learning_rate = 5e-5\n",
    "g_learning_rate = 5e-5\n",
    "\n",
    "clip = [-0.01, 0.01] \n",
    "\n",
    "d_steps = 5 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1 \n",
    "\n",
    "discriminator_activation_function = torch.sigmoid\n",
    "generator_activation_function = torch.sigmoid\n",
    "\n",
    "benign = torch.zeros([batch_size,1]).reshape(-1)\n",
    "mal = torch.ones([batch_size,1]).reshape(-1)\n",
    "\n",
    "G = Generator(input_size=g_input_size,\n",
    "              hidden_size=g_hidden_size,\n",
    "              output_size=g_output_size,\n",
    "              f=generator_activation_function)\n",
    "G.apply(init_weights)\n",
    "D = Discriminator(input_size=d_input_size,\n",
    "                  hidden_size=d_hidden_size,\n",
    "                  output_size=d_output_size,\n",
    "                  f=discriminator_activation_function)  \n",
    "D.apply(init_weights)\n",
    "d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "g_optimizer = optim.RMSprop(G.parameters(), lr=g_learning_rate)\n",
    "\n",
    "# Load the dataset\n",
    "xmal = load_data('Mirai_vector_mal.txt')\n",
    "ymal = np.ones((xmal.shape[0], 1), dtype=np.int)\n",
    "xben = load_data('Mirai_vector_ben.txt')\n",
    "yben = np.zeros((xben.shape[0], 1), dtype=np.int)\n",
    "# Malicious packets are divided into training set and test set\n",
    "train_size = int(xmal.shape[0] * 0.8)\n",
    "test_size = xmal.shape[0] - train_size \n",
    "xtrain_mal = xmal[0:train_size, :]\n",
    "xtest_mal = xmal[train_size:, :]\n",
    "ytrain_mal = ymal[0:train_size, :]\n",
    "ytest_mal = ymal[train_size:, :]\n",
    "# The benign packets are divided into training set and test set, the number of training sets is the same as the training set of malicious packets\n",
    "xtrain_ben = xben[0:train_size, :]\n",
    "xtest_ben = xben[train_size:, :]\n",
    "ytrain_ben = yben[0:train_size, :]\n",
    "ytest_ben = yben[train_size:, :]\n",
    "\n",
    "xtrain_mal = torch.Tensor(xtrain_mal)\n",
    "xtest_mal = torch.Tensor(xtest_mal)\n",
    "xtrain_ben = torch.Tensor(xtrain_ben)\n",
    "xtest_ben = torch.Tensor(xtest_ben)\n",
    "\n",
    "index_of_data_D_ben = 0     \n",
    "index_of_data_D_mal = 0    \n",
    "index_of_data_G = 0\n",
    "\n",
    "for epoch in range(1, epochs+1): \n",
    "    for d_index in range(d_steps):\n",
    "        D.zero_grad() # Zero gradient\n",
    "        \n",
    "        #  1A: Train D on real\n",
    "        idx = np.array(range(index_of_data_D_ben, index_of_data_D_ben+batch_size))\n",
    "        index_of_data_D_ben += batch_size\n",
    "        xben_batch = xtrain_ben[idx]\n",
    "        d_real_data = Variable(xben_batch)\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = torch.mean(d_real_decision.reshape(-1))\n",
    "        d_real_error.backward(benign.mean())\n",
    "        \n",
    "        #  1B: Train D on fake\n",
    "        idx = np.array(range(index_of_data_D_mal, index_of_data_D_mal+batch_size))\n",
    "        index_of_data_D_mal += batch_size\n",
    "        xmal_batch = xtrain_mal[idx]\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (batch_size, z_dims))\n",
    "        d_gen_input = Variable(torch.cat((xmal_batch, torch.Tensor(noise)), 1))\n",
    "        d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(d_fake_data)\n",
    "        d_fake_error = torch.mean(d_fake_decision.reshape(-1))\n",
    "        d_fake_error.backward(mal.mean())\n",
    "        \n",
    "        d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(clip[0], clip[1])\n",
    "            \n",
    "        dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "    \n",
    "    for g_index in range(g_steps):\n",
    "        G.zero_grad()\n",
    "        \n",
    "        idx = np.array(range(index_of_data_G, index_of_data_G+batch_size))\n",
    "        index_of_data_G += batch_size\n",
    "        xmal_batch = xtrain_mal[idx]\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (batch_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xmal_batch, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(gen_examples)\n",
    "        g_error = torch.mean(dg_fake_decision.reshape(-1))\n",
    "        g_error.backward(benign.mean())\n",
    "        g_loss = extract(g_error)[0]\n",
    "        \n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        # D(benign)\n",
    "        d_real_data = Variable(xtest_ben)\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = torch.mean(d_real_decision.reshape(-1))\n",
    "        d_loss = extract(d_real_error)[0]\n",
    "        #print(\"len of d_loss: %d\" % (len(d_loss)))\n",
    "        print(\"d_loss: %f\" % (d_loss))\n",
    "        # D(G(benign))\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (xben.shape[0] - train_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xtest_ben, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(Variable(gen_examples))\n",
    "        dg_error = torch.mean(dg_fake_decision.reshape(-1))\n",
    "        dg_loss = extract(dg_error)[0]\n",
    "        #print(\"len of dg_loss: %f\" % (len(dg_loss)))\n",
    "        print(\"dg_loss: %f\" % (dg_loss))\n",
    "        # Threshold\n",
    "#         threshold = 0\n",
    "#         for i in range(0, len(d_loss)):\n",
    "#             threshold = max(threshold, abs(d_loss[i]-dg_loss[i]))\n",
    "        threshold = abs(d_loss-dg_loss)\n",
    "        print(\"threshold: %f\" % (threshold)) \n",
    "        \n",
    "        \n",
    "#         # D(Malicious)\n",
    "#         d_real_data = Variable(xtest_mal)\n",
    "#         d_real_decision = D(d_real_data)\n",
    "#         d_real_error = d_real_decision.reshape(-1)\n",
    "#         d_loss = extract(d_real_error)\n",
    "#         print(\"len of d_loss: %d\" % (len(d_loss)))\n",
    "        # D(G(Malicious))\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (xmal.shape[0] - train_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xtest_mal, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(Variable(gen_examples))\n",
    "        dg_error = dg_fake_decision.reshape(-1)\n",
    "        dg_loss = extract(dg_error)\n",
    "        print(\"len of dg_loss: %f\" % (len(dg_loss)))\n",
    "        \n",
    "        # 计算 Precision Recall F-score\n",
    "        positive_sum = 6001\n",
    "        positive_num = 0\n",
    "        FP = 0\n",
    "        label_list = []\n",
    "        for i in range(0, len(dg_loss)):\n",
    "            if abs(d_loss-dg_loss[i]) > threshold:\n",
    "                label_list.append('1')\n",
    "                if ytest_mal[i][0] == 0:\n",
    "                    FP += 1\n",
    "                elif ytest_mal[i][0] == 1:\n",
    "                    positive_num += 1\n",
    "            else:\n",
    "                label_list.append('0')\n",
    "        \n",
    "\n",
    "        if positive_num == 0:\n",
    "            Precision = 0\n",
    "            Recall = 0\n",
    "            F_score = 0\n",
    "        else:\n",
    "            Precision = positive_num / (positive_num + FP)\n",
    "            Recall = positive_num / positive_sum\n",
    "            F_score = 2 * Recall * Precision / (Recall + Precision)\n",
    "        print('Precision: {0:0.4f}'.format(Precision)) \n",
    "        print('Recall: {0:0.4f}'.format(Recall)) \n",
    "        print('F-score: {0:0.4f}'.format(F_score)) \n",
    "        print(\"%d [D real_err: %f, fake_err: %f] [G loss: %f]\" % (epoch, dre, dfe, g_loss))          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6001\n"
     ]
    }
   ],
   "source": [
    "print(len(ytest_mal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No random seed\n",
    "Precision: 1.0000\n",
    "Recall: 0.9183\n",
    "F-score: 0.9574\n",
    "30 [D real_err: -0.144911, fake_err: -0.190961] [G loss: -0.191560]\n",
    "#\n",
    "torch.manual_seed(200)\n",
    "Precision: 1.0000\n",
    "Recall: 0.8794\n",
    "F-score: 0.9358\n",
    "10 [D real_err: -0.053383, fake_err: -0.081003] [G loss: -0.082181]\n",
    "# *\n",
    "torch.manual_seed(125)\n",
    "Precision: 1.0000\n",
    "Recall: 0.9257\n",
    "F-score: 0.9614\n",
    "50 [D real_err: -0.231154, fake_err: -0.315947] [G loss: -0.317548]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
