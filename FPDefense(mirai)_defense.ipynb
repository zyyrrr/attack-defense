{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mirai_gen_train.tsv, Mirai_gen_validation.tsv, Mirai_gen_test.tsv merged into Mirai_gen_mal.tsv\n",
    "import csv\n",
    "f = open('Mirai_gen_mal.tsv','w')\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "with open('Mirai_gen_train.tsv', 'r', encoding=\"utf8\") as x,open('Mirai_gen_validation.tsv', 'r', encoding=\"utf8\") as y,open('Mirai_gen_test.tsv', 'r', encoding=\"utf8\") as z:\n",
    "    x_reader = csv.reader(x, delimiter='\\t')\n",
    "    y_reader = csv.reader(y, delimiter='\\t')\n",
    "    z_reader = csv.reader(z, delimiter='\\t')\n",
    "    for x_row in x_reader:\n",
    "        writer.writerow(x_row)\n",
    "    for y_row in y_reader:\n",
    "        writer.writerow(y_row)\n",
    "    for z_row in z_reader:\n",
    "        writer.writerow(z_row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IEI = 8\n",
    "E = 1 / IEI \n",
    "packets_tsv = '100000_packets_mirai_time_{0}*{1}.tsv'.format(E,IEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30001\n",
      "100001\n",
      "100000\n",
      "merge success\n",
      "130002\n",
      "130001\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Extract the timestamp of the 100000 expanded packets\n",
    "list_all_t = []\n",
    "with open(packets_tsv, 'r', encoding=\"utf8\") as tsv_in:\n",
    "    tsv_reader = csv.reader(tsv_in, delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        list_all_t.append(row)\n",
    "                \n",
    "import random\n",
    "import torch\n",
    "torch.manual_seed(125) \n",
    "\n",
    "# First modify the timestamps of all adversarial samples\n",
    "list_m = []\n",
    "with open('Mirai_gen_mal.tsv', 'r', encoding=\"utf8\") as tsv_in: \n",
    "    tsv_reader = csv.reader(tsv_in, delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        list_m.append(row)\n",
    "\n",
    "#Randomly insert fake packets (note: insert the training set and test set separately to ensure that the sample size of the test set accounts for 20%)\n",
    "# end = float(list_all_t[-1][0])\n",
    "# start = float(list_all_t[-1 * int(E*IEI) * (30001) - 1][0]) \n",
    "# for i in range(0, len(list_m)): \n",
    "#     list_m[i][0] = random.uniform(start,end)\n",
    "start_train = float(list_all_t[-1 * int(E*IEI) * 30001][0]) \n",
    "end_train = float(list_all_t[-1 * int(E*IEI) * (6001) - 1][0]) \n",
    "for i in range(6001, len(list_m)): \n",
    "    list_m[i][0] = random.uniform(start_train,end_train)\n",
    "start_test = float(list_all_t[-1 * int(E*IEI) * 6001][0]) \n",
    "end_test = float(list_all_t[-1][0])\n",
    "for i in range(0, 6001): \n",
    "    list_m[i][0] = random.uniform(start_test,end_test)\n",
    "\n",
    "# Get the first element of the list\n",
    "def takeFirst(elem):\n",
    "    return elem[0]\n",
    "list_m.sort(key=takeFirst)\n",
    "    \n",
    "f = open('Mirai_gen_mal_modify_time_epoch.tsv','w') \n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "for m_row in list_m:\n",
    "    writer.writerow(m_row)\n",
    "f.close()\n",
    "\n",
    "######################################################################################  \n",
    "# Then insert all adversarial samples with modified timestamps into the original 100,000 data packets\n",
    "# At the same time, insert the labels of all adversarial samples into the original 100,000 packet label files\n",
    "list_x = []\n",
    "list_y = []\n",
    "list_z = []\n",
    "list_k = []\n",
    "# Can't write a line, add ‘\\’ to wrap\n",
    "with open('Mirai_gen_mal_modify_time_epoch.tsv', 'r', encoding=\"utf8\") as x,\\\n",
    "open(packets_tsv, 'r', encoding=\"utf8\") as y,\\\n",
    "open('100000_labels_mirai.tsv', 'r', encoding=\"utf8\") as z: \n",
    "    x_reader = csv.reader(x, delimiter='\\t')\n",
    "    y_reader = csv.reader(y, delimiter='\\t')\n",
    "    z_reader = csv.reader(z, delimiter='\\t')\n",
    "    for x_row in x_reader:\n",
    "        list_x.append(x_row)\n",
    "        list_k.append('2')\n",
    "    for y_row in y_reader:\n",
    "        list_y.append(y_row)\n",
    "    for z_row in z_reader:\n",
    "        list_z.append(z_row) \n",
    "gen_samples_num = len(list_x)\n",
    "print(len(list_x))\n",
    "print(len(list_y))\n",
    "print(len(list_z))\n",
    "list_p = []\n",
    "list_l = []\n",
    "list_p.append(list_y[0]) # The first line is the field name\n",
    "del list_y[0]\n",
    "while len(list_x) > 0 and len(list_y) > 0:\n",
    "    if list_x[0][0] < list_y[0][0]:\n",
    "        list_p.append(list_x[0])\n",
    "        del list_x[0]\n",
    "        list_l.append(list_k[0])\n",
    "        del list_k[0]\n",
    "    else:\n",
    "        list_p.append(list_y[0])\n",
    "        del list_y[0]\n",
    "        list_l.append(list_z[0])\n",
    "        del list_z[0]\n",
    "list_p.extend(list_x)\n",
    "list_p.extend(list_y)\n",
    "list_l.extend(list_k)\n",
    "list_l.extend(list_z)\n",
    "print(\"merge success\")\n",
    "print(len(list_p))\n",
    "print(len(list_l))\n",
    "    \n",
    "f = open('130001_packets_mirai(insert_all_FP).tsv','w')\n",
    "writer = csv.writer(f, delimiter='\\t')\n",
    "fla = open('130001_labels_mirai(insert_all_FP).tsv','w')\n",
    "writerla = csv.writer(fla, delimiter='\\t')\n",
    "for p_row in list_p:\n",
    "    writer.writerow(p_row)\n",
    "for l_row in list_l:\n",
    "    writerla.writerow(l_row)\n",
    "f.close()\n",
    "fla.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame.time_epoch', 'frame.len', 'eth.src', 'eth.dst', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport', 'udp.srcport', 'udp.dstport', 'icmp.type', 'icmp.code', 'arp.opcode', 'arp.src.hw_mac', 'arp.src.proto_ipv4', 'arp.dst.hw_mac', 'arp.dst.proto_ipv4', 'ipv6.src', 'ipv6.dst']\n",
      "max_vector_len=1350\n"
     ]
    }
   ],
   "source": [
    "def encode(s):\n",
    "    return ' '.join([bin(ord(c)).replace('0b', '') for c in s])\n",
    "\n",
    "def find_all(sub,s):\n",
    "    index_list = []\n",
    "    index = s.find(sub)\n",
    "    while index != -1:\n",
    "        index_list.append(index)\n",
    "        index = s.find(sub,index+1)\n",
    "        \n",
    "    if len(index_list) > 0:\n",
    "        return index_list\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_next_vector():\n",
    "    row = tsv_reader.__next__()\n",
    "    #print(row)\n",
    "    rowStr = ','.join(row)\n",
    "    #print(rowStr)\n",
    "    rowStrEnc = encode(rowStr)\n",
    "#     print(rowStrEnc)\n",
    "#     print()\n",
    "    index_list = find_all(' 101100',rowStrEnc) #find ','\n",
    "    mask = '0'\n",
    "    mask = mask.ljust(len(rowStrEnc),'0')\n",
    "    mask = list(mask)\n",
    "    modify_idx = [2,3,10,11,12,13,14,15,16,17,18] \n",
    "    #modify_idx = [2,3,4,5,14,16,17,18] \n",
    "    for i in modify_idx:\n",
    "        if i == 0:\n",
    "            head = 0\n",
    "        else:\n",
    "            head = index_list[i-1]+len(' 101100')\n",
    "        if i == 18:\n",
    "            tail = len(rowStrEnc)\n",
    "        else:\n",
    "            tail = index_list[i]\n",
    "        for j in range(head,tail):\n",
    "            mask[j]='1'\n",
    "    index_list = find_all(' 111010',rowStrEnc) #find ':'\n",
    "    for i in index_list:\n",
    "        head = i\n",
    "        tail = head+len(' 111010')\n",
    "        for j in range(head,tail):\n",
    "            mask[j]='0'\n",
    "    index_list = find_all(' 101110',rowStrEnc) #find '.'\n",
    "    for i in index_list:\n",
    "        head = i\n",
    "        tail = head+len(' 101110')\n",
    "        for j in range(head,tail):\n",
    "            mask[j]='0'\n",
    "    mask = ''.join(mask)\n",
    "#     print(mask)\n",
    "#     print()\n",
    "    row_enc_mask = rowStrEnc\n",
    "    row_enc_mask = list(row_enc_mask)\n",
    "    for i in range(len(rowStrEnc)):\n",
    "        if rowStrEnc[i]==' ':\n",
    "            row_enc_mask[i]='0'\n",
    "        elif mask[i]=='0':\n",
    "            row_enc_mask[i]='0'\n",
    "    row_enc_mask=''.join(row_enc_mask)\n",
    "#     print(row_enc_mask)\n",
    "#     print()\n",
    "    return row_enc_mask\n",
    "\n",
    "f = open('Mirai_vector_130001.txt','w')\n",
    "with open('130001_packets_mirai(insert_all_FP).tsv', 'r', encoding=\"utf8\") as tsv_in:\n",
    "    tsv_reader = csv.reader(tsv_in, delimiter='\\t')\n",
    "    row = tsv_reader.__next__()\n",
    "    print(row)\n",
    "    for i in range(0,130001): \n",
    "        vector = get_next_vector()\n",
    "        f.write(vector+'\\n')\n",
    "f.close()\n",
    "\n",
    "#Find vector maximum\n",
    "max_vector_len = 0\n",
    "for line in open(\"Mirai_vector_130001.txt\"):\n",
    "    max_vector_len = max(max_vector_len,len(line)-1) #Not including'\\n'\n",
    "print(\"max_vector_len={0}\".format(max_vector_len))\n",
    "\n",
    "#The length is unified to the maximum value, and mal and ben are stored separately\n",
    "f1 = open('Mirai_vector_mix_60002.txt','w')\n",
    "f2 = open('Mirai_vector_ben_69999.txt','w')\n",
    "with open(\"Mirai_vector_130001.txt\",'r') as y:\n",
    "    line2=y.readlines()\n",
    "    for i in range(0, 130001): \n",
    "        ss=line2[i]\n",
    "        ss=ss.strip('\\n')\n",
    "        ss=ss.ljust(max_vector_len,'0')\n",
    "        if i < 69999: \n",
    "            f2.write(ss+'\\n')\n",
    "        else:\n",
    "            f1.write(ss+'\\n')\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of yben: 69999\n",
      "train_size: 24000\n",
      "mix_train_size: 48000\n",
      "len of xtrain_mal: 24000\n",
      "len of xtest_mal: 12002\n",
      "len of mal: 6001\n",
      "len of dis: 6001\n",
      "d_loss: -0.092868\n",
      "dg_loss: -0.148406\n",
      "threshold: 0.055538\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.2854\n",
      "Recall: 0.2468\n",
      "F-score: 0.2647\n",
      "MRR: 5510 - 1481 / 5510 = 0.7312\n",
      "10 [D real_err: -0.092279, fake_err: -0.146997] [G loss: -0.148408]\n",
      "d_loss: -0.130221\n",
      "dg_loss: -0.198314\n",
      "threshold: 0.068093\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3018\n",
      "Recall: 0.2693\n",
      "F-score: 0.2846\n",
      "MRR: 5510 - 1616 / 5510 = 0.7067\n",
      "20 [D real_err: -0.128558, fake_err: -0.197547] [G loss: -0.198325]\n",
      "d_loss: -0.161208\n",
      "dg_loss: -0.231878\n",
      "threshold: 0.070670\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3191\n",
      "Recall: 0.3096\n",
      "F-score: 0.3143\n",
      "MRR: 5510 - 1858 / 5510 = 0.6628\n",
      "30 [D real_err: -0.160437, fake_err: -0.231222] [G loss: -0.231885]\n",
      "d_loss: -0.196145\n",
      "dg_loss: -0.274450\n",
      "threshold: 0.078305\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.5556\n",
      "Recall: 0.8910\n",
      "F-score: 0.6844\n",
      "MRR: 5510 - 5347 / 5510 = 0.0296\n",
      "40 [D real_err: -0.194581, fake_err: -0.273236] [G loss: -0.274457]\n",
      "d_loss: -0.245166\n",
      "dg_loss: -0.357638\n",
      "threshold: 0.112472\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.5631\n",
      "Recall: 0.9607\n",
      "F-score: 0.7100\n",
      "MRR: 5510 - 5765 / 5510 = -0.0463\n",
      "50 [D real_err: -0.244772, fake_err: -0.355752] [G loss: -0.357657]\n",
      "d_loss: -0.280396\n",
      "dg_loss: -0.427059\n",
      "threshold: 0.146663\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.4355\n",
      "Recall: 0.5892\n",
      "F-score: 0.5008\n",
      "MRR: 5510 - 3536 / 5510 = 0.3583\n",
      "60 [D real_err: -0.287173, fake_err: -0.426453] [G loss: -0.427075]\n",
      "d_loss: -0.289027\n",
      "dg_loss: -0.444309\n",
      "threshold: 0.155282\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3566\n",
      "Recall: 0.4189\n",
      "F-score: 0.3853\n",
      "MRR: 5510 - 2514 / 5510 = 0.5437\n",
      "70 [D real_err: -0.295407, fake_err: -0.444116] [G loss: -0.444317]\n",
      "d_loss: -0.294746\n",
      "dg_loss: -0.452243\n",
      "threshold: 0.157497\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3387\n",
      "Recall: 0.3863\n",
      "F-score: 0.3609\n",
      "MRR: 5510 - 2318 / 5510 = 0.5793\n",
      "80 [D real_err: -0.293645, fake_err: -0.452123] [G loss: -0.452248]\n",
      "d_loss: -0.299691\n",
      "dg_loss: -0.457149\n",
      "threshold: 0.157459\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3012\n",
      "Recall: 0.3114\n",
      "F-score: 0.3062\n",
      "MRR: 5510 - 1869 / 5510 = 0.6608\n",
      "90 [D real_err: -0.301343, fake_err: -0.457073] [G loss: -0.457156]\n",
      "d_loss: -0.304151\n",
      "dg_loss: -0.460432\n",
      "threshold: 0.156281\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3025\n",
      "Recall: 0.3178\n",
      "F-score: 0.3100\n",
      "MRR: 5510 - 1907 / 5510 = 0.6539\n",
      "100 [D real_err: -0.311297, fake_err: -0.460379] [G loss: -0.460434]\n",
      "d_loss: -0.308291\n",
      "dg_loss: -0.462741\n",
      "threshold: 0.154450\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3304\n",
      "Recall: 0.3764\n",
      "F-score: 0.3519\n",
      "MRR: 5510 - 2259 / 5510 = 0.5900\n",
      "110 [D real_err: -0.322622, fake_err: -0.462704] [G loss: -0.462745]\n",
      "d_loss: -0.312184\n",
      "dg_loss: -0.464421\n",
      "threshold: 0.152237\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3240\n",
      "Recall: 0.3644\n",
      "F-score: 0.3430\n",
      "MRR: 5510 - 2187 / 5510 = 0.6031\n",
      "120 [D real_err: -0.313167, fake_err: -0.464393] [G loss: -0.464425]\n",
      "d_loss: -0.315886\n",
      "dg_loss: -0.465669\n",
      "threshold: 0.149783\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3550\n",
      "Recall: 0.4349\n",
      "F-score: 0.3909\n",
      "MRR: 5510 - 2610 / 5510 = 0.5263\n",
      "130 [D real_err: -0.317096, fake_err: -0.465650] [G loss: -0.465671]\n",
      "d_loss: -0.319419\n",
      "dg_loss: -0.466613\n",
      "threshold: 0.147194\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.2924\n",
      "Recall: 0.2981\n",
      "F-score: 0.2952\n",
      "MRR: 5510 - 1789 / 5510 = 0.6753\n",
      "140 [D real_err: -0.319231, fake_err: -0.466599] [G loss: -0.466613]\n",
      "d_loss: -0.322767\n",
      "dg_loss: -0.467335\n",
      "threshold: 0.144568\n",
      "len of dg_loss: 12002\n",
      "Precision: 0.3023\n",
      "Recall: 0.2593\n",
      "F-score: 0.2791\n",
      "MRR: 5510 - 1556 / 5510 = 0.7176\n",
      "150 [D real_err: -0.319536, fake_err: -0.467322] [G loss: -0.467332]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(50)\n",
    "\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=1)\n",
    "        \n",
    "class Generator(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size, f): \n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map4 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        return self.f(self.map4(x))\n",
    "\n",
    "class Discriminator(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        return self.map3(x)\n",
    "\n",
    "def load_data(filename):\n",
    "    x = []\n",
    "    with open(filename,'r') as data:\n",
    "        lines = data.readlines()\n",
    "        for line in lines:\n",
    "            line=line.strip('\\n')\n",
    "            x.append(list(line))\n",
    "    x = np.array(x,dtype=np.float64)\n",
    "    return x\n",
    "\n",
    "# Model parameters\n",
    "feature_dims = max_vector_len\n",
    "z_dims = 20\n",
    "g_input_size = feature_dims + z_dims\n",
    "g_output_size = feature_dims\n",
    "g_hidden_size = int(math.sqrt(g_input_size+g_output_size)+10)\n",
    "d_input_size = g_output_size\n",
    "d_output_size = 1\n",
    "d_hidden_size = int(math.sqrt(d_input_size+d_output_size)+10)\n",
    "\n",
    "# 150*32*5=24000<=30001*0.8=24000\n",
    "epochs = 150 \n",
    "batch_size = 32\n",
    "print_interval = 10\n",
    "\n",
    "d_learning_rate = 5e-5\n",
    "g_learning_rate = 5e-5\n",
    "\n",
    "clip = [-0.01, 0.01] \n",
    "\n",
    "d_steps = 5 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1\n",
    "\n",
    "discriminator_activation_function = torch.sigmoid\n",
    "generator_activation_function = torch.sigmoid\n",
    "\n",
    "benign = torch.zeros([batch_size,1]).reshape(-1)\n",
    "mal = torch.ones([batch_size,1]).reshape(-1)\n",
    "\n",
    "G = Generator(input_size=g_input_size,\n",
    "              hidden_size=g_hidden_size,\n",
    "              output_size=g_output_size,\n",
    "              f=generator_activation_function)\n",
    "G.apply(init_weights)\n",
    "D = Discriminator(input_size=d_input_size,\n",
    "                  hidden_size=d_hidden_size,\n",
    "                  output_size=d_output_size,\n",
    "                  f=discriminator_activation_function)  \n",
    "D.apply(init_weights)\n",
    "d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)\n",
    "g_optimizer = optim.RMSprop(G.parameters(), lr=g_learning_rate)\n",
    "\n",
    "# Load the dataset\n",
    "xmal = load_data('Mirai_vector_mix_60002.txt')\n",
    "ymal = []\n",
    "with open(\"130001_labels_mirai(insert_all_FP).tsv\",'r') as x:\n",
    "    line1=x.readlines()\n",
    "    for i in range(69999, 130001): \n",
    "        if line1[i].find('1') != -1:\n",
    "            ymal.append(1)\n",
    "        elif line1[i].find('2') != -1:\n",
    "            ymal.append(2)\n",
    "ymal = np.array(ymal)[:, np.newaxis] \n",
    "xben = load_data('Mirai_vector_ben_69999.txt')\n",
    "yben = np.zeros((xben.shape[0], 1), dtype=np.int)\n",
    "print(\"len of yben: %d\" % (yben.shape[0]))\n",
    "\n",
    "# Malicious packets are divided into training set and test set\n",
    "train_size = int(30001 * 0.8) \n",
    "cnt = 0\n",
    "mix_train_size = 0\n",
    "xmal_list = xmal.tolist()\n",
    "xtrain_mal = []\n",
    "with open(\"130001_labels_mirai(insert_all_FP).tsv\",'r') as x:\n",
    "    line1=x.readlines()\n",
    "    for i in range(69999, 130001): \n",
    "        mix_train_size += 1\n",
    "        if line1[i].find('1') != -1:\n",
    "            cnt += 1\n",
    "            xtrain_mal.append(xmal_list[i - 69999]) \n",
    "        if cnt == train_size:\n",
    "            break\n",
    "xtrain_mal = np.array(xtrain_mal)\n",
    "print(\"train_size: %d\" % (train_size))\n",
    "print(\"mix_train_size: %d\" % (mix_train_size))\n",
    "xtest_mal = xmal[mix_train_size:, :]\n",
    "ytrain_mal = np.ones((train_size, 1), dtype=np.int)\n",
    "ytest_mal = ymal[mix_train_size:, :]\n",
    "print(\"len of xtrain_mal: %d\" % (xtrain_mal.shape[0]))\n",
    "print(\"len of xtest_mal: %d\" % (xtest_mal.shape[0]))\n",
    "mal_tot = 0\n",
    "dis_tot = 0\n",
    "for array in ytest_mal:\n",
    "    if array[0] == 1:\n",
    "        mal_tot += 1\n",
    "    elif array[0] == 2:\n",
    "        dis_tot += 1\n",
    "print(\"len of mal: %d\" % (mal_tot))\n",
    "print(\"len of dis: %d\" % (dis_tot))\n",
    "# The benign packets are divided into training set and test set, the number of training sets is the same as the training set of malicious packets\n",
    "xtrain_ben = xben[0:train_size, :]\n",
    "xtest_ben = xben[train_size:, :]\n",
    "ytrain_ben = yben[0:train_size, :]\n",
    "ytest_ben = yben[train_size:, :]\n",
    "\n",
    "xtrain_mal = torch.Tensor(xtrain_mal)\n",
    "xtest_mal = torch.Tensor(xtest_mal)\n",
    "xtrain_ben = torch.Tensor(xtrain_ben)\n",
    "xtest_ben = torch.Tensor(xtest_ben)\n",
    "\n",
    "index_of_data_D_ben = 0     \n",
    "index_of_data_D_mal = 0    \n",
    "index_of_data_G = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for d_index in range(d_steps):\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #  1A: Train D on real\n",
    "        idx = np.array(range(index_of_data_D_ben, index_of_data_D_ben+batch_size))\n",
    "        index_of_data_D_ben += batch_size\n",
    "        xben_batch = xtrain_ben[idx]\n",
    "        d_real_data = Variable(xben_batch)\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = torch.mean(d_real_decision.reshape(-1))\n",
    "        d_real_error.backward(benign.mean())\n",
    "        \n",
    "        #  1B: Train D on fake\n",
    "        idx = np.array(range(index_of_data_D_mal, index_of_data_D_mal+batch_size))\n",
    "        index_of_data_D_mal += batch_size\n",
    "        xmal_batch = xtrain_mal[idx]\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (batch_size, z_dims))\n",
    "        d_gen_input = Variable(torch.cat((xmal_batch, torch.Tensor(noise)), 1))\n",
    "        d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels\n",
    "        d_fake_decision = D(d_fake_data)\n",
    "        d_fake_error = torch.mean(d_fake_decision.reshape(-1))\n",
    "        d_fake_error.backward(mal.mean())\n",
    "        \n",
    "        d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(clip[0], clip[1])\n",
    "            \n",
    "        dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "    \n",
    "    for g_index in range(g_steps):\n",
    "        G.zero_grad()\n",
    "        \n",
    "        idx = np.array(range(index_of_data_G, index_of_data_G+batch_size))\n",
    "        index_of_data_G += batch_size\n",
    "        xmal_batch = xtrain_mal[idx]\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (batch_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xmal_batch, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(gen_examples)\n",
    "        g_error = torch.mean(dg_fake_decision.reshape(-1))\n",
    "        g_error.backward(benign.mean())\n",
    "        g_loss = extract(g_error)[0]\n",
    "        \n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        # D(benign)\n",
    "        d_real_data = Variable(xtest_ben)\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = torch.mean(d_real_decision.reshape(-1))\n",
    "        d_loss = extract(d_real_error)[0]\n",
    "        #print(\"len of d_loss: %d\" % (len(d_loss)))\n",
    "        print(\"d_loss: %f\" % (d_loss))\n",
    "        # D(G(benign))\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (xben.shape[0] - train_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xtest_ben, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(Variable(gen_examples))\n",
    "        dg_error = torch.mean(dg_fake_decision.reshape(-1))\n",
    "        dg_loss = extract(dg_error)[0]\n",
    "        #print(\"len of dg_loss: %f\" % (len(dg_loss)))\n",
    "        print(\"dg_loss: %f\" % (dg_loss))\n",
    "\n",
    "#         threshold = 0\n",
    "#         for i in range(0, len(d_loss)):\n",
    "#             threshold = max(threshold, abs(d_loss[i]-dg_loss[i]))\n",
    "        threshold = abs(d_loss-dg_loss)\n",
    "        print(\"threshold: %f\" % (threshold)) \n",
    "        \n",
    "        # D(G(Malicious))\n",
    "        np.random.seed(0)\n",
    "        noise = np.random.uniform(0, 1, (xmal.shape[0] - mix_train_size, z_dims))\n",
    "        gen_input = Variable(torch.cat((xtest_mal, torch.Tensor(noise)), 1))\n",
    "        gen_examples = G(gen_input)\n",
    "        dg_fake_decision = D(Variable(gen_examples))\n",
    "        dg_error = dg_fake_decision.reshape(-1)\n",
    "        dg_loss = extract(dg_error)\n",
    "        print(\"len of dg_loss: %d\" % (len(dg_loss)))\n",
    "        \n",
    "        # Calculation Precision Recall F-score MRR\n",
    "        positive_sum = 6001\n",
    "        positive_num = 0\n",
    "        FP = 0\n",
    "        label_list = []\n",
    "        for i in range(0, len(dg_loss)):\n",
    "            if abs(d_loss-dg_loss[i]) > threshold:\n",
    "                label_list.append('1')\n",
    "                if ytest_mal[i][0] == 0 or ytest_mal[i][0] == 2:\n",
    "                    FP += 1\n",
    "                elif ytest_mal[i][0] == 1:\n",
    "                    positive_num += 1\n",
    "            else:\n",
    "                label_list.append('0')\n",
    "        \n",
    "\n",
    "        label_str = ''.join(label_list)\n",
    "        file = open('6001_mirai_labels_gan.txt','a')\n",
    "        file.write(label_str + '\\n')\n",
    "        file.close()\n",
    "        \n",
    "        if positive_num == 0:\n",
    "            Precision = 0\n",
    "            Recall = 0\n",
    "            F_score = 0\n",
    "        else:\n",
    "            Precision = positive_num / (positive_num + FP)\n",
    "            Recall = positive_num / positive_sum\n",
    "            F_score = 2 * Recall * Precision / (Recall + Precision)\n",
    "        origin_mal_num = 5510 \n",
    "        MRR = (origin_mal_num - positive_num) / origin_mal_num\n",
    "        print('Precision: {0:0.4f}'.format(Precision)) \n",
    "        print('Recall: {0:0.4f}'.format(Recall)) \n",
    "        print('F-score: {0:0.4f}'.format(F_score)) \n",
    "        print('MRR: {0} - {1} / {2} = {3:0.4f}'.format(origin_mal_num,positive_num,origin_mal_num,MRR))\n",
    "        print(\"%d [D real_err: %f, fake_err: %f] [G loss: %f]\" % (epoch, dre, dfe, g_loss)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "torch.manual_seed(125)\n",
    "d_loss: -0.209761\n",
    "dg_loss: -0.296548\n",
    "threshold: 0.086788\n",
    "len of dg_loss: 12002\n",
    "Precision: 0.6444\n",
    "Recall: 0.9723\n",
    "F-score: 0.7751\n",
    "MRR: 5510 - 5835 / 5510 = -0.0590\n",
    "40 [D real_err: -0.208606, fake_err: -0.295453] [G loss: -0.296602]\n",
    "#\n",
    "torch.manual_seed(0)\n",
    "d_loss: -0.315643\n",
    "dg_loss: -0.465664\n",
    "threshold: 0.150022\n",
    "len of dg_loss: 12002\n",
    "Precision: 0.5888\n",
    "Recall: 0.8775\n",
    "F-score: 0.7047\n",
    "MRR: 5510 - 5266 / 5510 = 0.0443\n",
    "130 [D real_err: -0.316925, fake_err: -0.465663] [G loss: -0.465679]\n",
    "# *\n",
    "torch.manual_seed(200)\n",
    "d_loss: -0.321863\n",
    "dg_loss: -0.467227\n",
    "threshold: 0.145364\n",
    "len of dg_loss: 12002\n",
    "Precision: 0.5418\n",
    "Recall: 0.9733\n",
    "F-score: 0.6961\n",
    "MRR: 5510 - 5841 / 5510 = -0.0601\n",
    "150 [D real_err: -0.318696, fake_err: -0.467217] [G loss: -0.467229]\n",
    "#\n",
    "torch.manual_seed(100)\n",
    "d_loss: -0.240253\n",
    "dg_loss: -0.349383\n",
    "threshold: 0.109130\n",
    "len of dg_loss: 12002\n",
    "Precision: 0.6299\n",
    "Recall: 0.9265\n",
    "F-score: 0.7499\n",
    "MRR: 5510 - 5560 / 5510 = -0.0091\n",
    "50 [D real_err: -0.239902, fake_err: -0.347397] [G loss: -0.349398]\n",
    "#\n",
    "torch.manual_seed(50)\n",
    "d_loss: -0.245166\n",
    "dg_loss: -0.357638\n",
    "threshold: 0.112472\n",
    "len of dg_loss: 12002\n",
    "Precision: 0.5631\n",
    "Recall: 0.9607\n",
    "F-score: 0.7100\n",
    "MRR: 5510 - 5765 / 5510 = -0.0463\n",
    "50 [D real_err: -0.244772, fake_err: -0.355752] [G loss: -0.357657]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the fake packets as malicious packets, the result is unstable, hereby record (no random seed is set)\n",
    "#11111111111111111111111111111111111\n",
    "d_loss: -0.322740\n",
    "dg_loss: -0.467157\n",
    "threshold: 0.144417\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 5788 / 6001 = 0.9645\n",
    "dis_TPR: 2946 / 4187 = 0.7036\n",
    "dis_mal_TPR: 8734 / 10188 = 0.8573\n",
    "MRR: 5510 - 5788 / 5510 = -0.0505\n",
    "150 [D real_err: -0.318539, fake_err: -0.467148] [G loss: -0.467162]\n",
    "#22222222222222222222222222222222222\n",
    "d_loss: -0.322835\n",
    "dg_loss: -0.467304\n",
    "threshold: 0.144469\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 282 / 6001 = 0.0470\n",
    "dis_TPR: 1693 / 4187 = 0.4043\n",
    "dis_mal_TPR: 1975 / 10188 = 0.1939\n",
    "MRR: 5510 - 282 / 5510 = 0.9488\n",
    "150 [D real_err: -0.318586, fake_err: -0.467286] [G loss: -0.467302]\n",
    "#333333333333333333333333333333333333\n",
    "d_loss: -0.321234\n",
    "dg_loss: -0.467102\n",
    "threshold: 0.145868\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 5663 / 6001 = 0.9437\n",
    "dis_TPR: 4017 / 4187 = 0.9594\n",
    "dis_mal_TPR: 9680 / 10188 = 0.9501\n",
    "MRR: 5510 - 5663 / 5510 = -0.0278\n",
    "150 [D real_err: -0.316993, fake_err: -0.467095] [G loss: -0.467110]\n",
    "#44444444444444444444444444444444444\n",
    "d_loss: -0.324198\n",
    "dg_loss: -0.467553\n",
    "threshold: 0.143356\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 202 / 6001 = 0.0337\n",
    "dis_TPR: 1205 / 4187 = 0.2878\n",
    "dis_mal_TPR: 1407 / 10188 = 0.1381\n",
    "MRR: 5510 - 202 / 5510 = 0.9633\n",
    "150 [D real_err: -0.319922, fake_err: -0.467534] [G loss: -0.467549]\n",
    "#55555555555555555555555555555555555\n",
    "d_loss: -0.322354\n",
    "dg_loss: -0.467157\n",
    "threshold: 0.144803\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 83 / 6001 = 0.0138\n",
    "dis_TPR: 1203 / 4187 = 0.2873\n",
    "dis_mal_TPR: 1286 / 10188 = 0.1262\n",
    "MRR: 5510 - 83 / 5510 = 0.9849\n",
    "150 [D real_err: -0.318029, fake_err: -0.467139] [G loss: -0.467153]\n",
    "#66666666666666666666666666666666666\n",
    "d_loss: -0.322957\n",
    "dg_loss: -0.467438\n",
    "threshold: 0.144481\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 4360 / 6001 = 0.7265\n",
    "dis_TPR: 1770 / 4187 = 0.4227\n",
    "dis_mal_TPR: 6130 / 10188 = 0.6017\n",
    "MRR: 5510 - 4360 / 5510 = 0.2087\n",
    "150 [D real_err: -0.318654, fake_err: -0.467423] [G loss: -0.467437]\n",
    "#77777777777777777777777777777777777\n",
    "d_loss: -0.322979\n",
    "dg_loss: -0.467327\n",
    "threshold: 0.144348\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 4930 / 6001 = 0.8215\n",
    "dis_TPR: 992 / 4187 = 0.2369\n",
    "dis_mal_TPR: 5922 / 10188 = 0.5813\n",
    "MRR: 5510 - 4930 / 5510 = 0.1053\n",
    "150 [D real_err: -0.318737, fake_err: -0.467315] [G loss: -0.467326]\n",
    "#88888888888888888888888888888888888\n",
    "d_loss: -0.322789\n",
    "dg_loss: -0.467399\n",
    "threshold: 0.144610\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 49 / 6001 = 0.0082\n",
    "dis_TPR: 64 / 4187 = 0.0153\n",
    "dis_mal_TPR: 113 / 10188 = 0.0111\n",
    "MRR: 5510 - 49 / 5510 = 0.9911\n",
    "150 [D real_err: -0.318498, fake_err: -0.467379] [G loss: -0.467392]\n",
    "#99999999999999999999999999999999999\n",
    "d_loss: -0.323224\n",
    "dg_loss: -0.467356\n",
    "threshold: 0.144132\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 1548 / 6001 = 0.2580\n",
    "dis_TPR: 292 / 4187 = 0.0697\n",
    "dis_mal_TPR: 1840 / 10188 = 0.1806\n",
    "MRR: 5510 - 1548 / 5510 = 0.7191\n",
    "150 [D real_err: -0.318809, fake_err: -0.467338] [G loss: -0.467349]\n",
    "#00000000000000000000000000000000000\n",
    "d_loss: -0.323670\n",
    "dg_loss: -0.467358\n",
    "threshold: 0.143687\n",
    "len of dg_loss: 10188.000000\n",
    "mal_TPR: 5727 / 6001 = 0.9543\n",
    "dis_TPR: 1380 / 4187 = 0.3296\n",
    "dis_mal_TPR: 7107 / 10188 = 0.6976\n",
    "MRR: 5510 - 5727 / 5510 = -0.0394\n",
    "150 [D real_err: -0.319449, fake_err: -0.467346] [G loss: -0.467357]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
